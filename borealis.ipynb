{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2aRQ8sD2tLL"
      },
      "source": [
        "Subset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DkcLftAD2tLM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def subset(file_paths=[]):\n",
        "\n",
        "    dataframes = [pd.read_csv(file) for file in file_paths]\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NSEWijaT30VX",
        "outputId": "d2f2f655-b7c0-469f-fedc-09ade4fd9e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LdanCNU2tLN"
      },
      "source": [
        "Dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-InYUrBM2tLN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def dataset():\n",
        "    file_paths = [\n",
        "        '/content/drive/MyDrive/combined_features_with_date1.csv',\n",
        "        # '/content/drive/MyDrive/combined_features_with_date2.csv',\n",
        "        # '/content/drive/MyDrive/combined_features_with_date3.csv'\n",
        "    ]\n",
        "\n",
        "    Dataset = subset(file_paths=file_paths)\n",
        "\n",
        "    TrainingData, TestData = train_test_split(Dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "    TrainingData.to_csv(\"TrainingData.csv\", index=False)\n",
        "    TestData.to_csv(\"TestData.csv\", index=False)\n",
        "\n",
        "    with open(\"features_info.txt\", \"w\") as f:\n",
        "        f.write(\"Features used:\\n\")\n",
        "        for feature in Dataset.columns:\n",
        "            f.write(f\"{feature}\\n\")\n",
        "\n",
        "    print(\"Datasets have been successfully split and saved as 'TrainingData.csv' and 'TestData.csv'.\")\n",
        "    print(\"Features info saved as 'features_info.txt'.\")\n",
        "\n",
        "    return TrainingData, TestData\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLmQ_h72tLN"
      },
      "source": [
        "neuralnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FWkFm8RF2tLO",
        "outputId": "9f6864b0-6a39-4838-bdb1-9f167bca1d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 7.0806 - mae: 1.6215\n",
            "Epoch 1: val_loss improved from inf to 8.09063, saving model to /content/checkpoints/model_epoch_01.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 7.9026 - mae: 1.6776 - val_loss: 8.0906 - val_mae: 1.6326\n",
            "Epoch 2/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 5.8985 - mae: 1.5911\n",
            "Epoch 2: val_loss improved from 8.09063 to 7.47118, saving model to /content/checkpoints/model_epoch_02.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.1574 - mae: 1.6028 - val_loss: 7.4712 - val_mae: 1.5684\n",
            "Epoch 3/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.6809 - mae: 0.9391\n",
            "Epoch 3: val_loss improved from 7.47118 to 6.94973, saving model to /content/checkpoints/model_epoch_03.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.4538 - mae: 1.3486 - val_loss: 6.9497 - val_mae: 1.5183\n",
            "Epoch 4/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3910 - mae: 1.4315\n",
            "Epoch 4: val_loss improved from 6.94973 to 6.48767, saving model to /content/checkpoints/model_epoch_04.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1936 - mae: 1.5019 - val_loss: 6.4877 - val_mae: 1.4897\n",
            "Epoch 5/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9912 - mae: 0.8141\n",
            "Epoch 5: val_loss improved from 6.48767 to 6.09524, saving model to /content/checkpoints/model_epoch_05.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6323 - mae: 1.2747 - val_loss: 6.0952 - val_mae: 1.4769\n",
            "Epoch 6/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.7791 - mae: 1.3117\n",
            "Epoch 6: val_loss improved from 6.09524 to 5.74541, saving model to /content/checkpoints/model_epoch_06.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.0500 - mae: 1.4660 - val_loss: 5.7454 - val_mae: 1.4631\n",
            "Epoch 7/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6944 - mae: 1.5782\n",
            "Epoch 7: val_loss improved from 5.74541 to 5.43918, saving model to /content/checkpoints/model_epoch_07.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.9802 - mae: 1.4815 - val_loss: 5.4392 - val_mae: 1.4388\n",
            "Epoch 8/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.7276 - mae: 1.2686\n",
            "Epoch 8: val_loss improved from 5.43918 to 5.17892, saving model to /content/checkpoints/model_epoch_08.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7611 - mae: 1.3651 - val_loss: 5.1789 - val_mae: 1.4097\n",
            "Epoch 9/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.4271 - mae: 1.3538\n",
            "Epoch 9: val_loss improved from 5.17892 to 4.93874, saving model to /content/checkpoints/model_epoch_09.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2716 - mae: 1.3823 - val_loss: 4.9387 - val_mae: 1.3783\n",
            "Epoch 10/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.9874 - mae: 1.2707\n",
            "Epoch 10: val_loss improved from 4.93874 to 4.72341, saving model to /content/checkpoints/model_epoch_10.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.3693 - mae: 1.3024 - val_loss: 4.7234 - val_mae: 1.3399\n",
            "Model training complete.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def build_model(input_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_size,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def prepare_dataset(data, batch_size=32, shuffle=True):\n",
        "    # Separate features and target\n",
        "    X = data.drop(columns=[\"streamflow\"]).values\n",
        "    y = data[\"streamflow\"].values\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Create tf.data.Dataset from the arrays\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # Shuffle and batch the dataset\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset, scaler\n",
        "\n",
        "def train_nn(TrainingData, epochs=10, batch_size=32, checkpoint_dir=\"/content/checkpoints\"):\n",
        "    # Ensure checkpoint directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Update checkpoint path to end in `.weights.h5`\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"model_epoch_{epoch:02d}.weights.h5\")\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True, # Save only the best model\n",
        "        monitor='val_loss',   # Monitor validation loss\n",
        "        mode='min',           # Save model with minimum validation loss\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Remove unnecessary columns and NaNs\n",
        "    if 'date' in TrainingData.columns:\n",
        "        TrainingData = TrainingData.drop(columns=['date'])\n",
        "    TrainingData = TrainingData.dropna()\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset, scaler = prepare_dataset(TrainingData, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Build the model\n",
        "    input_size = TrainingData.drop(columns=[\"streamflow\"]).shape[1]\n",
        "    model = build_model(input_size)\n",
        "\n",
        "    # Train the model with checkpointing\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=train_dataset.take(20),  # Small validation split\n",
        "        callbacks=[checkpoint_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Model training complete.\")\n",
        "    return model, scaler\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load training data\n",
        "    # train_data, validation_data, test_data = dataset()\n",
        "    train_data = pd.read_csv('/content/TrainingData.csv').sample(10000, random_state=1)\n",
        "    NNModel, scaler = train_nn(train_data)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}