{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2aRQ8sD2tLL"
      },
      "source": [
        "Subset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DkcLftAD2tLM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def subset(file_paths=[]):\n",
        "\n",
        "    dataframes = [pd.read_csv(file) for file in file_paths]\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NSEWijaT30VX",
        "outputId": "d2f2f655-b7c0-469f-fedc-09ade4fd9e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LdanCNU2tLN"
      },
      "source": [
        "Dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-InYUrBM2tLN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def dataset():\n",
        "    file_paths = [\n",
        "        '/content/drive/MyDrive/combined_features_with_date1.csv',\n",
        "        # '/content/drive/MyDrive/combined_features_with_date2.csv',\n",
        "        # '/content/drive/MyDrive/combined_features_with_date3.csv'\n",
        "    ]\n",
        "\n",
        "    Dataset = subset(file_paths=file_paths)\n",
        "\n",
        "    TrainingData, TestData = train_test_split(Dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "    TrainingData.to_csv(\"TrainingData.csv\", index=False)\n",
        "    TestData.to_csv(\"TestData.csv\", index=False)\n",
        "\n",
        "    with open(\"features_info.txt\", \"w\") as f:\n",
        "        f.write(\"Features used:\\n\")\n",
        "        for feature in Dataset.columns:\n",
        "            f.write(f\"{feature}\\n\")\n",
        "\n",
        "    print(\"Datasets have been successfully split and saved as 'TrainingData.csv' and 'TestData.csv'.\")\n",
        "    print(\"Features info saved as 'features_info.txt'.\")\n",
        "\n",
        "    return TrainingData, TestData\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQLmQ_h72tLN"
      },
      "source": [
        "neuralnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FWkFm8RF2tLO",
        "outputId": "9f6864b0-6a39-4838-bdb1-9f167bca1d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - loss: 7.0806 - mae: 1.6215\n",
            "Epoch 1: val_loss improved from inf to 8.09063, saving model to /content/checkpoints/model_epoch_01.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - loss: 7.9026 - mae: 1.6776 - val_loss: 8.0906 - val_mae: 1.6326\n",
            "Epoch 2/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 5.8985 - mae: 1.5911\n",
            "Epoch 2: val_loss improved from 8.09063 to 7.47118, saving model to /content/checkpoints/model_epoch_02.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 7.1574 - mae: 1.6028 - val_loss: 7.4712 - val_mae: 1.5684\n",
            "Epoch 3/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 1.6809 - mae: 0.9391\n",
            "Epoch 3: val_loss improved from 7.47118 to 6.94973, saving model to /content/checkpoints/model_epoch_03.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 5.4538 - mae: 1.3486 - val_loss: 6.9497 - val_mae: 1.5183\n",
            "Epoch 4/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 7.3910 - mae: 1.4315\n",
            "Epoch 4: val_loss improved from 6.94973 to 6.48767, saving model to /content/checkpoints/model_epoch_04.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 7.1936 - mae: 1.5019 - val_loss: 6.4877 - val_mae: 1.4897\n",
            "Epoch 5/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.9912 - mae: 0.8141\n",
            "Epoch 5: val_loss improved from 6.48767 to 6.09524, saving model to /content/checkpoints/model_epoch_05.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 4.6323 - mae: 1.2747 - val_loss: 6.0952 - val_mae: 1.4769\n",
            "Epoch 6/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.7791 - mae: 1.3117\n",
            "Epoch 6: val_loss improved from 6.09524 to 5.74541, saving model to /content/checkpoints/model_epoch_06.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.0500 - mae: 1.4660 - val_loss: 5.7454 - val_mae: 1.4631\n",
            "Epoch 7/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6.6944 - mae: 1.5782\n",
            "Epoch 7: val_loss improved from 5.74541 to 5.43918, saving model to /content/checkpoints/model_epoch_07.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.9802 - mae: 1.4815 - val_loss: 5.4392 - val_mae: 1.4388\n",
            "Epoch 8/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.7276 - mae: 1.2686\n",
            "Epoch 8: val_loss improved from 5.43918 to 5.17892, saving model to /content/checkpoints/model_epoch_08.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.7611 - mae: 1.3651 - val_loss: 5.1789 - val_mae: 1.4097\n",
            "Epoch 9/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 5.4271 - mae: 1.3538\n",
            "Epoch 9: val_loss improved from 5.17892 to 4.93874, saving model to /content/checkpoints/model_epoch_09.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 5.2716 - mae: 1.3823 - val_loss: 4.9387 - val_mae: 1.3783\n",
            "Epoch 10/10\n",
            "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.9874 - mae: 1.2707\n",
            "Epoch 10: val_loss improved from 4.93874 to 4.72341, saving model to /content/checkpoints/model_epoch_10.weights.h5\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.3693 - mae: 1.3024 - val_loss: 4.7234 - val_mae: 1.3399\n",
            "Model training complete.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def build_model(input_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_size,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def prepare_dataset(data, batch_size=32, shuffle=True):\n",
        "    X = data.drop(columns=[\"streamflow\"]).values\n",
        "    y = data[\"streamflow\"].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(X))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset, scaler\n",
        "\n",
        "def train_nn(TrainingData, epochs=10, batch_size=32, checkpoint_dir=\"/content/checkpoints\"):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"model_epoch_{epoch:02d}.weights.h5\")\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Remove unnecessary columns and NaNs\n",
        "    if 'date' in TrainingData.columns:\n",
        "        TrainingData = TrainingData.drop(columns=['date'])\n",
        "    TrainingData = TrainingData.dropna()\n",
        "\n",
        "    train_dataset, scaler = prepare_dataset(TrainingData, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    input_size = TrainingData.drop(columns=[\"streamflow\"]).shape[1]\n",
        "    model = build_model(input_size)\n",
        "\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=train_dataset.take(20),\n",
        "        callbacks=[checkpoint_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"Model training complete.\")\n",
        "    return model, scaler\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # train_data, validation_data, test_data = dataset\n",
        "    train_data = pd.read_csv('/content/TrainingData.csv').sample(10000, random_state=1)\n",
        "    NNModel, scaler = train_nn(train_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time Series Forecasting:"
      ],
      "metadata": {
        "id": "R8HU64OWogrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup"
      ],
      "metadata": {
        "id": "vfbdJ298oix0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoAGSAeErymJ",
        "outputId": "399a3a5c-4246-4e59-9c7c-73dea9bf5825"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/combined_features_with_date1.csv'\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "date_time = df.pop('date')\n",
        "\n",
        "# print(df.head())\n",
        "# df.describe().transpose()\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igVq1RGgolWQ",
        "outputId": "a3d01dc6-c815-439e-dfaf-b2c1a72ec8ec"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['snow_depth_water_equivalent_mean', 'surface_net_solar_radiation_mean',\n",
            "       'surface_net_thermal_radiation_mean', 'surface_pressure_mean',\n",
            "       'temperature_2m_mean', 'dewpoint_temperature_2m_mean',\n",
            "       'u_component_of_wind_10m_mean', 'v_component_of_wind_10m_mean',\n",
            "       'volumetric_soil_water_layer_1_mean',\n",
            "       'volumetric_soil_water_layer_2_mean',\n",
            "       'volumetric_soil_water_layer_3_mean',\n",
            "       'volumetric_soil_water_layer_4_mean', 'snow_depth_water_equivalent_max',\n",
            "       'surface_net_solar_radiation_max', 'surface_net_thermal_radiation_max',\n",
            "       'surface_pressure_max', 'temperature_2m_max',\n",
            "       'volumetric_soil_water_layer_1_min', 'total_precipitation_sum',\n",
            "       'potential_evaporation_sum', 'streamflow'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
        "\n",
        "\n",
        "year = 365.2425 * 24 * 60 * 60  # seconds in a year\n",
        "\n",
        "# Add sine and cosine transforms for yearly periodicity\n",
        "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
        "\n",
        "\n",
        "print(df[['Year sin', 'Year cos']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPxIHxQV1oDI",
        "outputId": "f1a946bc-bd0f-4712-d1df-d85e02740234"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year sin  Year cos\n",
            "0 -0.006752  0.999977\n",
            "1  0.010450  0.999945\n",
            "2  0.027650  0.999618\n",
            "3  0.044841  0.998994\n",
            "4  0.062019  0.998075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "\n",
        "train_df = df[0:int(n * 0.7)]  # First 70% of the data\n",
        "val_df = df[int(n * 0.7):int(n * 0.9)]  # Next 20% of the data\n",
        "test_df = df[int(n * 0.9):]  # Last 10% of the data\n",
        "\n",
        "num_features = df.shape[1]"
      ],
      "metadata": {
        "id": "CUcKqIYRIbVB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean and standard deviation of the training data\n",
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "\n",
        "# Normalize the datasets\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "val_df = (val_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std"
      ],
      "metadata": {
        "id": "_VQaWaegKjvG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])"
      ],
      "metadata": {
        "id": "pYthDnfML05R"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_width = 7       # Number of days in the input\n",
        "label_width = 1       # Predict 1 day\n",
        "shift = 1             # Offset of 1 day into the future\n",
        "label_columns = [\"streamflow\"]  # Target column\n",
        "\n",
        "w2 = WindowGenerator(\n",
        "    input_width=input_width,\n",
        "    label_width=label_width,\n",
        "    shift=shift,\n",
        "    train_df=train_df,\n",
        "    val_df=val_df,\n",
        "    test_df=test_df,\n",
        "    label_columns=label_columns\n",
        ")\n",
        "\n",
        "print(w2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBWS5NnSMhYZ",
        "outputId": "de1c4fc4-5136-4aaf-cb36-86ec7d467d9b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total window size: 8\n",
            "Input indices: [0 1 2 3 4 5 6]\n",
            "Label indices: [7]\n",
            "Label column name(s): ['streamflow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes\n",
        "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window\n"
      ],
      "metadata": {
        "id": "VTtE3YFFO1ea"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack three slices, the length of the total window.\n",
        "example_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n",
        "                           np.array(train_df[100:100+w2.total_window_size]),\n",
        "                           np.array(train_df[200:200+w2.total_window_size])])\n",
        "\n",
        "example_inputs, example_labels = w2.split_window(example_window)\n",
        "\n",
        "print('All shapes are: (batch, time, features)')\n",
        "print(f'Window shape: {example_window.shape}')\n",
        "print(f'Inputs shape: {example_inputs.shape}')\n",
        "print(f'Labels shape: {example_labels.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e_rqa3mO_9r",
        "outputId": "25fb517d-fae2-4e3f-a1a6-1c9bb57fab0d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All shapes are: (batch, time, features)\n",
            "Window shape: (3, 8, 23)\n",
            "Inputs shape: (3, 7, 23)\n",
            "Labels shape: (3, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2.example = example_inputs, example_labels"
      ],
      "metadata": {
        "id": "nsuWopacQQU4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(self, data):\n",
        "    # Convert to NumPy array\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "\n",
        "    # Create the dataset with overlapping windows\n",
        "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=None,  # We handle labels separately in `split_window`\n",
        "        sequence_length=self.total_window_size,  # Full window size\n",
        "        sequence_stride=1,  # Slide by one step\n",
        "        shuffle=True,  # Shuffle windows\n",
        "        batch_size=32,  # Number of windows in each batch\n",
        "    )\n",
        "\n",
        "    # Map to (inputs, labels) using the `split_window` method\n",
        "    ds = ds.map(self.split_window)\n",
        "\n",
        "    return ds\n",
        "\n",
        "# Add the method to the WindowGenerator class\n",
        "WindowGenerator.make_dataset = make_dataset\n"
      ],
      "metadata": {
        "id": "bUI9ljzrUs13"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@property\n",
        "def train(self):\n",
        "    return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def val(self):\n",
        "    return self.make_dataset(self.val_df)\n",
        "\n",
        "@property\n",
        "def test(self):\n",
        "    return self.make_dataset(self.test_df)\n",
        "\n",
        "@property\n",
        "def example(self):\n",
        "  result = getattr(self, '_example', None)\n",
        "  if result is None:\n",
        "\n",
        "    result = next(iter(self.train))\n",
        "    print(f\"Train dataset output: {next(iter(self.train))}\")\n",
        "    self._example = result\n",
        "  return result\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.val = val\n",
        "WindowGenerator.test = test\n",
        "WindowGenerator.example = test"
      ],
      "metadata": {
        "id": "HEUSD380U-oT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(self, model=None, plot_col='streamflow', max_subplots=3):\n",
        "\n",
        "  print(f\"self.example: {self.example}\")\n",
        "  print(f\"Type of self.example: {type(self.example)}\")\n",
        "  inputs, labels = self.example\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plot_col_index = self.column_indices[plot_col]\n",
        "  max_n = min(max_subplots, len(inputs))\n",
        "  for n in range(max_n):\n",
        "    plt.subplot(max_n, 1, n+1)\n",
        "    plt.ylabel(f'{plot_col} [normed]')\n",
        "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "             label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "    if self.label_columns:\n",
        "      label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "    else:\n",
        "      label_col_index = plot_col_index\n",
        "\n",
        "    if label_col_index is None:\n",
        "      continue\n",
        "\n",
        "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "    if model is not None:\n",
        "      predictions = model(inputs)\n",
        "      plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                  marker='X', edgecolors='k', label='Predictions',\n",
        "                  c='#ff7f0e', s=64)\n",
        "\n",
        "    if n == 0:\n",
        "      plt.legend()\n",
        "\n",
        "  plt.xlabel('Time [h]')\n",
        "\n",
        "WindowGenerator.plot = plot"
      ],
      "metadata": {
        "id": "G6Za9snnQRKP"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "collapsed": true,
        "id": "iE9F70ruQUNU",
        "outputId": "62df56a6-0f5e-44ed-beba-ca6870ee4bc0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self.example: <_MapDataset element_spec=(TensorSpec(shape=(None, 7, 23), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))>\n",
            "Type of self.example: <class 'tensorflow.python.data.ops.map_op._MapDataset'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-066934815ba1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-d6f4bd0ce780>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, model, plot_col, max_subplots)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"self.example: {self.example}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Type of self.example: {type(self.example)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mplot_col_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2.train.element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHNYLq0oVPdO",
        "outputId": "1f616348-7fba-4560-90a1-a861df243e8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, 7, 23), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None, 1, 1), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seven_day_window = WindowGenerator(\n",
        "    input_width=7,  # Number of days as input\n",
        "    label_width=1,  # Single day to predict\n",
        "    shift=1,        # Predict the day immediately following the 7-day input\n",
        "    label_columns=['streamflow']  # Column to predict\n",
        ")\n",
        "\n",
        "# Print window configuration for verification\n",
        "print(seven_day_window)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ozgIj-sY2p1",
        "outputId": "cd773187-fac4-45ee-ce72-be542060e7a4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total window size: 8\n",
            "Input indices: [0 1 2 3 4 5 6]\n",
            "Label indices: [7]\n",
            "Label column name(s): ['streamflow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example_inputs, example_labels in seven_day_window.train.take(1):\n",
        "    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "    print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ1ZbNZTY9Qb",
        "outputId": "6c01d521-b626-4adc-c6b9-4180432b53b1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape (batch, time, features): (32, 7, 23)\n",
            "Labels shape (batch, time, features): (32, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check one batch from the train dataset\n",
        "for example_inputs, example_labels in seven_day_window.train.take(1):\n",
        "    print(f\"Example inputs: {example_inputs.numpy()}\")\n",
        "    print(f\"Example labels: {example_labels.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsT40sH6ZFb9",
        "outputId": "0a36c49c-4f6b-4b12-d07e-1e6887ebcd0d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example inputs: [[[-4.85611051e-01  3.11668277e-01 -1.04319680e+00 ... -5.67949653e-01\n",
            "   -1.40676367e+00  2.18415875e-02]\n",
            "  [-4.85611051e-01  3.09577465e-01 -5.75602829e-01 ... -5.71919024e-01\n",
            "   -1.40627337e+00  4.61921208e-02]\n",
            "  [-4.85611051e-01  1.24726556e-01 -6.79855824e-01 ... -5.79857767e-01\n",
            "   -1.40536499e+00  7.05306008e-02]\n",
            "  ...\n",
            "  [-4.85611051e-01  3.57665777e-01 -1.13509393e+00 ... -5.87796509e-01\n",
            "   -1.40229607e+00  1.19142577e-01]\n",
            "  [-4.85611051e-01 -1.21104574e+00  1.29284251e+00 ... -5.91765881e-01\n",
            "   -1.40013623e+00  1.43401697e-01]\n",
            "  [-4.85611051e-01 -7.79111981e-01  1.43570769e+00 ... -5.87796509e-01\n",
            "   -1.39756048e+00  1.67619973e-01]]\n",
            "\n",
            " [[-4.85611051e-01 -1.63311258e-01  3.47615331e-01 ... -3.29787314e-01\n",
            "   -1.09281339e-01 -1.40565181e+00]\n",
            "  [-4.85611051e-01  1.97175980e+00 -1.54476953e+00 ... -3.41695428e-01\n",
            "   -1.33484185e-01 -1.40345883e+00]\n",
            "  [-4.85611051e-01  1.43848395e+00 -5.66335917e-01 ... -3.61542284e-01\n",
            "   -1.57645807e-01 -1.40084887e+00]\n",
            "  ...\n",
            "  [-4.85611051e-01  1.89870501e+00 -1.53511655e+00 ... -3.81389141e-01\n",
            "   -2.05816805e-01 -1.39438128e+00]\n",
            "  [-4.85611051e-01  9.77894068e-01 -6.32748961e-01 ... -3.89327884e-01\n",
            "   -2.29811922e-01 -1.39052570e+00]\n",
            "  [-4.85611051e-01  1.84778798e+00 -1.00999033e+00 ... -3.81389141e-01\n",
            "   -2.53737301e-01 -1.38625693e+00]]\n",
            "\n",
            " [[-4.85611051e-01  1.44487941e+00 -1.59264874e+00 ... -5.52072167e-01\n",
            "   -2.78300077e-01 -1.38142967e+00]\n",
            "  [-4.85611051e-01  1.72443104e+00 -1.98031545e+00 ... -5.56041539e-01\n",
            "   -3.02062154e-01 -1.37632632e+00]\n",
            "  [-4.85611051e-01  1.90731418e+00 -1.95792031e+00 ... -5.56041539e-01\n",
            "   -3.25733155e-01 -1.37081409e+00]\n",
            "  ...\n",
            "  [-4.85611051e-01  1.15757954e+00 -7.67119467e-01 ... -5.52072167e-01\n",
            "   -3.72773796e-01 -1.35856950e+00]\n",
            "  [-4.85611051e-01  6.99203372e-01 -2.85238951e-01 ... -5.56041539e-01\n",
            "   -3.96129549e-01 -1.35184085e+00]\n",
            "  [-4.85611051e-01  8.18501651e-01 -1.44304350e-01 ... -4.92531568e-01\n",
            "   -4.19366330e-01 -1.34471047e+00]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.53884515e-01 -1.34756231e+00  1.88940132e+00 ... -2.22614273e-01\n",
            "    5.24335504e-01  1.32243085e+00]\n",
            "  [ 2.33239383e-01 -1.20219064e+00  1.50134850e+00 ... -2.34522387e-01\n",
            "    5.46863258e-01  1.31329656e+00]\n",
            "  [ 2.64864922e-01 -1.02644074e+00  5.41448653e-01 ... -2.38491759e-01\n",
            "    5.69230914e-01  1.30377531e+00]\n",
            "  ...\n",
            "  [ 2.64670908e-01 -7.24628329e-01 -1.11926293e+00 ... -2.58338630e-01\n",
            "    6.13459408e-01  1.28358293e+00]\n",
            "  [ 2.65252978e-01 -1.09371519e+00  1.02950716e+00 ... -2.70246744e-01\n",
            "    6.35307193e-01  1.27291799e+00]\n",
            "  [ 2.73983955e-01 -1.28274775e+00  1.75116956e+00 ... -2.78185487e-01\n",
            "    6.56968713e-01  1.26187789e+00]]\n",
            "\n",
            " [[-2.90231198e-01 -1.06407511e+00  2.77727216e-01 ... -1.39257476e-01\n",
            "   -1.74199164e-01  1.40971184e+00]\n",
            "  [-2.62874126e-01 -8.86972427e-01  4.44145888e-01 ... -1.78951189e-01\n",
            "   -1.50069967e-01  1.41260743e+00]\n",
            "  [-2.58799672e-01 -1.17513323e+00  9.66183066e-01 ... -2.14675531e-01\n",
            "   -1.25894651e-01  1.41508663e+00]\n",
            "  ...\n",
            "  [-3.70328575e-02 -1.15287244e+00  8.06328475e-01 ... -1.07502490e-01\n",
            "   -7.74342343e-02  1.41879308e+00]\n",
            "  [ 1.96550763e-03 -1.30082691e+00  1.31330693e+00 ... -1.94828674e-01\n",
            "   -5.31634837e-02  1.42001915e+00]\n",
            "  [ 3.76655012e-02 -8.55364561e-01 -4.30807024e-01 ... -1.78951189e-01\n",
            "   -2.88752858e-02  1.42082655e+00]]\n",
            "\n",
            " [[ 2.63445210e+00 -5.40146351e-01  1.19747031e+00 ...  1.92878520e+00\n",
            "    1.41090941e+00 -1.40957907e-01]\n",
            "  [ 2.34477758e+00 -6.35339081e-01  1.75657523e+00 ...  1.68268418e+00\n",
            "    1.40818858e+00 -1.65160269e-01]\n",
            "  [ 1.89678156e+00  2.48452455e-01 -1.64768830e-01 ...  2.36541605e+00\n",
            "    1.40505290e+00 -1.89312145e-01]\n",
            "  ...\n",
            "  [ 1.71575928e+00  2.66408682e-01 -2.92189151e-01 ...  2.84967947e+00\n",
            "    1.39754009e+00 -2.37435862e-01]\n",
            "  [ 1.69635713e+00  7.92551219e-01 -1.67257607e+00 ...  2.23839617e+00\n",
            "    1.39316535e+00 -2.61393458e-01]\n",
            "  [ 1.70043159e+00  3.34052086e-01 -3.09950769e-01 ...  1.90893841e+00\n",
            "    1.38838005e+00 -2.85272121e-01]]]\n",
            "Example labels: [[[-0.5877965 ]]\n",
            "\n",
            " [[-0.3734504 ]]\n",
            "\n",
            " [[-0.5242865 ]]\n",
            "\n",
            " [[-0.18688993]]\n",
            "\n",
            " [[-0.2781855 ]]\n",
            "\n",
            " [[-0.22658364]]\n",
            "\n",
            " [[ 0.18226165]]\n",
            "\n",
            " [[-0.48459283]]\n",
            "\n",
            " [[-0.5917659 ]]\n",
            "\n",
            " [[-0.51634777]]\n",
            "\n",
            " [[-0.17101245]]\n",
            "\n",
            " [[-0.5242865 ]]\n",
            "\n",
            " [[ 1.5000931 ]]\n",
            "\n",
            " [[-0.57985777]]\n",
            "\n",
            " [[-0.5004703 ]]\n",
            "\n",
            " [[-0.54016405]]\n",
            "\n",
            " [[-0.47665408]]\n",
            "\n",
            " [[ 0.22989412]]\n",
            "\n",
            " [[-0.5004703 ]]\n",
            "\n",
            " [[-0.13528809]]\n",
            "\n",
            " [[-0.22261427]]\n",
            "\n",
            " [[ 4.7629166 ]]\n",
            "\n",
            " [[ 3.8737774 ]]\n",
            "\n",
            " [[-0.36154228]]\n",
            "\n",
            " [[-0.28612423]]\n",
            "\n",
            " [[-0.40917474]]\n",
            "\n",
            " [[-0.57985777]]\n",
            "\n",
            " [[ 1.2023902 ]]\n",
            "\n",
            " [[-0.2781855 ]]\n",
            "\n",
            " [[-0.28215486]]\n",
            "\n",
            " [[-0.13528809]]\n",
            "\n",
            " [[ 1.7620716 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline"
      ],
      "metadata": {
        "id": "a9b_ArRnZgKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline(tf.keras.Model):\n",
        "    def __init__(self, label_index=None):\n",
        "        super().__init__()\n",
        "        self.label_index = label_index\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # If label_index is not provided, return all inputs\n",
        "        if self.label_index is None:\n",
        "            return inputs\n",
        "        # Extract the specific feature (e.g., streamflow)\n",
        "        result = inputs[:, -1, self.label_index]\n",
        "        return result[:, tf.newaxis, tf.newaxis]  # Shape: (batch_size, 1, 1)"
      ],
      "metadata": {
        "id": "MhetHCYoZhep"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming column_indices is a dictionary mapping column names to their indices\n",
        "baseline = Baseline(label_index=column_indices['streamflow'])\n",
        "\n",
        "# Compile the model with loss and metrics\n",
        "baseline.compile(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        ")"
      ],
      "metadata": {
        "id": "xS9AQ1x5bAjj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_performance = {}\n",
        "performance = {}\n",
        "\n",
        "# Evaluate on validation and test datasets\n",
        "val_performance['Baseline'] = baseline.evaluate(seven_day_window.val, return_dict=True)\n",
        "performance['Baseline'] = baseline.evaluate(seven_day_window.test, verbose=0, return_dict=True)\n",
        "\n",
        "print(\"Validation Performance:\", val_performance['Baseline'])\n",
        "print(\"Test Performance:\", performance['Baseline'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc4QeGsXbD8c",
        "outputId": "fca577b9-a84a-4763-a461-9dfb4756198b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17808/17808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - loss: 0.8078 - mean_absolute_error: 0.2621\n",
            "Validation Performance: {'loss': 0.8022377490997314, 'mean_absolute_error': 0.26256343722343445}\n",
            "Test Performance: {'loss': 1.360756754875183, 'mean_absolute_error': 0.3725825250148773}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seven_day_window.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "hSI5RvcKZZGV",
        "outputId": "244a8200-ea01-440d-b861-56982ea83701"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-6675d26c929a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseven_day_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-5bc0a3d5d298>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, model, plot_col, max_subplots)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'streamflow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_subplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mplot_col_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplot_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmax_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_subplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wide_window = WindowGenerator(\n",
        "    input_width=24, label_width=24, shift=1,\n",
        "    label_columns=['streamflow'])\n",
        "\n",
        "wide_window"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0P-0o0Zbe15",
        "outputId": "8861851d-7860-426c-d403-366e7f47e9de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Total window size: 25\n",
              "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
              "Label indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
              "Label column name(s): ['streamflow']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear"
      ],
      "metadata": {
        "id": "lHEj_SGIbwQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "checkpoint_path = \"/content/checkpoints/best_model.weights.h5\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Loading best weights from:\", checkpoint_path)\n",
        "    linear.load_weights(checkpoint_path)\n",
        "else:\n",
        "    print(\"No saved weights found, training the model from scratch.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StHNdZTsbxjW",
        "outputId": "d18b21d7-d9c4-4fc2-ceea-646297345d12"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved weights found, training the model from scratch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCHS = 10\n",
        "\n",
        "\n",
        "def compile_and_fit(model, window, patience=2, checkpoint_dir=\"/content/checkpoints\"):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"best_model.weights.h5\")\n",
        "\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=patience,\n",
        "        mode='min',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        optimizer=tf.keras.optimizers.Adam(),\n",
        "        metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        window.train,\n",
        "        epochs=MAX_EPOCHS,\n",
        "        validation_data=window.val,\n",
        "        callbacks=[early_stopping, checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    print(f\"Model training complete. Best model saved at {checkpoint_path}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "Ji2XEiHec73b"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = compile_and_fit(linear, seven_day_window)\n",
        "\n",
        "checkpoint_path = \"/content/checkpoints/best_model.weights.h5\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(\"Loading best weights from:\", checkpoint_path)\n",
        "    linear.load_weights(checkpoint_path)\n",
        "else:\n",
        "    print(\"No saved weights found, training the model from scratch.\")\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "val_performance = {}\n",
        "performance = {}\n",
        "\n",
        "\n",
        "val_performance['Linear'] = linear.evaluate(seven_day_window.val, return_dict=True)\n",
        "performance['Linear'] = linear.evaluate(seven_day_window.test, verbose=0, return_dict=True)\n",
        "\n",
        "print(\"Validation Performance:\", val_performance)\n",
        "print(\"Test Performance:\", performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEihp1tpdT9O",
        "outputId": "112fae60-9fc5-49b5-b8cf-136b3fb2c106"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading best weights from: /content/checkpoints/best_model.weights.h5\n",
            "\u001b[1m17808/17808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 4ms/step - loss: 1.4454 - mean_absolute_error: 0.5390\n",
            "Validation Performance: {'Linear': {'loss': 1.410473346710205, 'mean_absolute_error': 0.5369311571121216}}\n",
            "Test Performance: {'Linear': {'loss': 2.227755069732666, 'mean_absolute_error': 0.6645160913467407}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a 1 day ahead prediction"
      ],
      "metadata": {
        "id": "PPWuvBK_e_na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the test dataset\n",
        "test_data = seven_day_window.test\n",
        "\n",
        "# Convert the test dataset to a Pandas DataFrame (only if the dataset is small enough)\n",
        "df = pd.DataFrame(list(test_data))\n",
        "\n",
        "# Select the last 7 rows\n",
        "last_7_days = df.tail(7)\n",
        "\n",
        "# Step 1: Extract the last 7 days from df\n",
        "last_7_days = df[-7:].copy()  # Make a copy to avoid modifying the original DataFrame\n",
        "\n",
        "# Step 2: Select the relevant columns for prediction\n",
        "last_7_days = last_7_days[['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'area']]\n",
        "\n",
        "# Step 3: Make predictions (example with a model)\n",
        "# Assuming you have a trained model 'model' to predict streamflow\n",
        "test_data = last_7_days.copy()  # Create a copy to keep the original intact\n",
        "test_data['predicted_streamflow'] = model.predict(last_7_days[['snow_depth', 'solar_radiation', 'temperature']])  # Example prediction\n",
        "\n",
        "# Step 4: Ensure the columns are correctly set\n",
        "test_data.columns = ['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'area', 'predicted_streamflow']\n",
        "\n",
        "# Step 5: Verify columns and data\n",
        "print(test_data.columns)  # Should show: ['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'area', 'predicted_streamflow']\n",
        "print(test_data.head())  # Check the first few rows of the data\n",
        "\n",
        "print(last_7_days.columns)\n",
        "# Proceed with the previous steps (dropping 'streamflow' and reshaping)\n",
        "# last_7_days_data = last_7_days.drop(columns='streamflow').values\n",
        "# last_7_days_data = last_7_days_data.reshape((1, 7, last_7_days_data.shape[1]))\n",
        "\n",
        "# # Make prediction\n",
        "# prediction = linear.predict(last_7_days_data)\n",
        "\n",
        "# print(\"Predicted streamflow for the next day: \", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "wxGmBmtHfBwD",
        "outputId": "52504e88-450c-4000-cfa3-a3d7a8df103a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'area'], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-50692da0e2b1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 2: Select the relevant columns for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlast_7_days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_7_days\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gauge_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gauge_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gauge_lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gauge_lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Step 3: Make predictions (example with a model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['gauge_id', 'gauge_name', 'gauge_lat', 'gauge_lon', 'area'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense"
      ],
      "metadata": {
        "id": "oVG5CufZd6u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "# history = compile_and_fit(dense, seven_day_window)\n",
        "\n",
        "# val_performance['Dense'] = dense.evaluate(seven_day_window.val, return_dict=True)\n",
        "# performance['Dense'] = dense.evaluate(seven_day_window.test, verbose=0, return_dict=True)\n",
        "\n",
        "print(\"Validation Performance:\", val_performance)\n",
        "print(\"Test Performance:\", performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThJEK2fqd6XI",
        "outputId": "1a408518-915e-42a9-f208-8f77675921a0"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Performance: {'Baseline': {'loss': 0.8022377490997314, 'mean_absolute_error': 0.26256343722343445}, 'Dense': {'loss': 1.296350359916687, 'mean_absolute_error': 0.5048043131828308}}\n",
            "Test Performance: {'Baseline': {'loss': 1.360756754875183, 'mean_absolute_error': 0.3725825250148773}, 'Dense': {'loss': 2.01128888130188, 'mean_absolute_error': 0.6455956101417542}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Step Dense"
      ],
      "metadata": {
        "id": "7ffe_FpdRYl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Input_width = 7\n",
        "Label_width = 1\n",
        "Shift = 1\n",
        "\n",
        "daily_window= WindowGenerator(\n",
        "    input_width=Input_width,\n",
        "    label_width= Label_width,\n",
        "    shift=Shift,\n",
        "    label_columns=[\"streamflow\"]\n",
        ")\n",
        "\n",
        "print(daily_window)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_jQgVLxQmSl",
        "outputId": "c114c37e-2558-4dca-afdf-b74c60a86aa4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total window size: 8\n",
            "Input indices: [0 1 2 3 4 5 6]\n",
            "Label indices: [7]\n",
            "Label column name(s): ['streamflow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_step_dense = tf.keras.Sequential([\n",
        "    # Shape: (time, features) => (time*features)\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=1),\n",
        "    # Add back the time dimension.\n",
        "    # Shape: (outputs) => (1, outputs)\n",
        "    tf.keras.layers.Reshape([1, -1]),\n",
        "])"
      ],
      "metadata": {
        "id": "7F8pXuUPTGKR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = compile_and_fit(multi_step_dense, daily_window)\n",
        "\n",
        "IPython.display.clear_output()\n",
        "val_performance['Multi step dense'] = multi_step_dense.evaluate(daily_window.val, return_dict=True)\n",
        "performance['Multi step dense'] = multi_step_dense.evaluate(daily_window.test, verbose=0, return_dict=True)\n",
        "\n",
        "print(\"Validation Performance:\", val_performance)\n",
        "print(\"Test Performance:\", performance)"
      ],
      "metadata": {
        "id": "VGrM0H8uTYZs",
        "outputId": "6959b1e6-3599-4b6e-f805-26d77dcc0bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17808/17808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3ms/step - loss: 0.4586 - mean_absolute_error: 0.2162\n",
            "Validation Performance: {'Baseline': {'loss': 0.8022377490997314, 'mean_absolute_error': 0.26256343722343445}, 'Dense': {'loss': 1.296350359916687, 'mean_absolute_error': 0.5048043131828308}, 'Multi step dense': {'loss': 0.4825913608074188, 'mean_absolute_error': 0.21780778467655182}}\n",
            "Test Performance: {'Baseline': {'loss': 1.360756754875183, 'mean_absolute_error': 0.3725825250148773}, 'Dense': {'loss': 2.01128888130188, 'mean_absolute_error': 0.6455956101417542}, 'Multi step dense': {'loss': 0.8236036896705627, 'mean_absolute_error': 0.3033800423145294}}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}